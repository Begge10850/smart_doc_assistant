import streamlit as st
import os
import asyncio
from io import BytesIO
from s3_upload import upload_to_s3
from rag_pipeline import download_file_from_s3, extract_text_from_file
from vector_store import chunk_text, embed_chunks, build_faiss_index
from qa_engine import search_index, answer_question_with_gpt

# â”€â”€â”€ Fix asyncio issue for Windows â”€â”€â”€
try:
    asyncio.get_running_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())

# â”€â”€â”€ Session Initialization â”€â”€â”€
if "doc_ready" not in st.session_state:
    st.session_state.doc_ready = False

# â”€â”€â”€ Page Config â”€â”€â”€
st.set_page_config(page_title="ğŸ“„ Saidia Smart Document Assistant", layout="wide")
st.title("ğŸ“„ Saidia Smart Document Assistant")
st.markdown("Upload a document, extract the text, and interact with it using smart search and GPT-powered Q&A.")

# â”€â”€â”€ Sidebar Upload â”€â”€â”€
with st.sidebar:
    st.header("ğŸ“¤ Upload Document")
    uploaded_file = st.file_uploader("Choose a .pdf, .txt, or .docx file", type=["pdf", "txt", "docx"])
    if uploaded_file and st.button("ğŸš€ Process Document"):
        st.session_state.doc_ready = True
        st.session_state.uploaded_file_name = uploaded_file.name
        st.session_state.uploaded_file_data = uploaded_file.read()

# â”€â”€â”€ Main Workflow â”€â”€â”€
if st.session_state.doc_ready and "uploaded_file_data" in st.session_state:
    uploaded_file = BytesIO(st.session_state.uploaded_file_data)
    uploaded_file.name = st.session_state.uploaded_file_name
    st.info(f"ğŸ“ File selected: `{uploaded_file.name}`")

    # Upload to S3
    result = upload_to_s3(st.session_state.uploaded_file_data, st.session_state.uploaded_file_name)
    if not result:
        st.error("âŒ Upload to S3 failed.")
        st.stop()
    st.success("âœ… Uploaded to S3")

    # Download to temp folder
    local_path = os.path.join("temp", uploaded_file.name)
    os.makedirs("temp", exist_ok=True)
    downloaded = download_file_from_s3(uploaded_file.name, local_path)
    if not downloaded:
        st.error("âŒ Download from S3 failed.")
        st.stop()
    st.success("ğŸ“¥ File downloaded from S3")

    # Extract text
    extracted_text = extract_text_from_file(local_path)
    st.success(f"ğŸ§¾ Extracted text length: {len(extracted_text)} characters")
    if not extracted_text.strip():
        st.warning("âš ï¸ No text extracted.")
        st.stop()

    # Show preview
    with st.expander("ğŸ§  Preview Extracted Text", expanded=False):
        st.text_area("Document Text", extracted_text, height=300)

    # Chunking
    chunks = chunk_text(extracted_text)
    st.info(f"ğŸ“„ Document split into **{len(chunks)}** chunks.")

    # Embedding
    embeddings = embed_chunks(chunks)
    st.success("ğŸ”¢ Chunks embedded.")

    # FAISS Index
    index = build_faiss_index(embeddings)
    st.success("ğŸ“¦ Vector index created and ready.")

    # Q&A
    with st.form(key="qa_form"):
        st.subheader("ğŸ’¬ Ask a Question About the Document")
        user_question = st.text_input("Type your question:")
        ask_button = st.form_submit_button("Ask GPT")

        if ask_button:
            if not user_question.strip():
                st.warning("âš ï¸ Please enter a question.")
            else:
                st.info("ğŸ” Generating answer...")
                try:
                    relevant_chunks = search_index(user_question, index, chunks)
                    answer = answer_question_with_gpt(user_question, relevant_chunks)
                    if answer:
                        st.success("âœ… Answer:")
                        st.write(answer)
                    else:
                        st.warning("âš ï¸ GPT returned no answer.")
                except Exception as e:
                    print("ğŸ›‘ Error during Q&A:", e)
                    st.error(f"âŒ An error occurred: {e}")
